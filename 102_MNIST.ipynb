{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train = np.load(\"MNIST_x_train.npy\")\n",
    "x_test = np.load(\"MNIST_x_test.npy\")\n",
    "y_train = np.load(\"MNIST_y_train.npy\")\n",
    "y_test = np.load(\"MNIST_y_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape,y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "x_train = np.reshape(x_train,(60000,-1))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input reshape \n",
    "# model 만들기 (비선형)\n",
    "# train, visualize 구현 -> tensor로 만들기\n",
    "# test 구현 -> test input 넣고 -> test_acc_hist 반환 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28 * 28, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 10),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        y = self.model(x)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train,(60000,-1))\n",
    "x_test = np.reshape(x_test,(10000,-1))\n",
    "x_train = torch.tensor(x_train,dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.long)\n",
    "y_test = torch.tensor(y_test,dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss,CrossEntropyLoss\n",
    "from torch.optim import SGD,Adam,RMSprop\n",
    "import copy\n",
    "\n",
    "model = MyModel()\n",
    "optimizer = Adam(model.parameters(),lr=0.01)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "num_epoch = 100\n",
    "model_hist=[]\n",
    "loss_hist=[]\n",
    "train_acc_hist=[]\n",
    "test_acc_hist=[]\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    global x_test,y_test,test_acc_hist\n",
    "    # model(x_test)\n",
    "    # max_idx = argmax\n",
    "    # acc = sum(y_test == max_idx) / len(y_test)\n",
    "    y_pred = model(x_test)\n",
    "    y_pred = torch.exp(y_pred)\n",
    "    y_pred = y_pred/(torch.sum(y_pred,dim=1)).unsqueeze(1)\n",
    "    pred_max_idx = torch.argmax(y_pred,1)\n",
    "    acc = sum(y_test==pred_max_idx).detach() / len(y_test)\n",
    "\n",
    "    return acc.item()\n",
    "\n",
    "def train():\n",
    "    global x_train,y_train,optimizer,loss_fn,model_hist,loss_hist,train_acc_hist,num_epoch\n",
    "\n",
    "    for epoch in range(1,num_epoch+1):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)\n",
    "        \n",
    "        #acc\n",
    "        pred_idx = torch.argmax(y_pred,1)\n",
    "        acc = sum(y_train==pred_idx).detach()/len(y_train)\n",
    "        \n",
    "        loss = loss_fn(y_pred,y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #test\n",
    "        test_acc = test()\n",
    "\n",
    "        if epoch%(num_epoch//10)==0:\n",
    "            print(f\"Epoch {epoch}: loss = {loss.item():.2f} acc = {acc.item():.2f}, test acc= {test_acc:.2f}\")\n",
    "            model_hist.append(copy.deepcopy(model))\n",
    "        \n",
    "        loss_hist.append(loss.item())\n",
    "        train_acc_hist.append(acc.item())\n",
    "\n",
    "    return loss_hist,model_hist,train_acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss = 0.12 acc = 0.96, test acc= 0.96\n",
      "Epoch 20: loss = 0.10 acc = 0.97, test acc= 0.96\n",
      "Epoch 30: loss = 0.09 acc = 0.97, test acc= 0.96\n",
      "Epoch 40: loss = 0.08 acc = 0.98, test acc= 0.96\n",
      "Epoch 50: loss = 0.07 acc = 0.98, test acc= 0.96\n",
      "Epoch 60: loss = 0.06 acc = 0.98, test acc= 0.96\n",
      "Epoch 70: loss = 0.06 acc = 0.98, test acc= 0.96\n",
      "Epoch 80: loss = 0.05 acc = 0.99, test acc= 0.97\n",
      "Epoch 90: loss = 0.04 acc = 0.99, test acc= 0.97\n",
      "Epoch 100: loss = 0.03 acc = 0.99, test acc= 0.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.305330753326416,\n",
       "  2.257697582244873,\n",
       "  2.1168155670166016,\n",
       "  1.909471035003662,\n",
       "  1.661846399307251,\n",
       "  1.3580609560012817,\n",
       "  1.0672671794891357,\n",
       "  0.9130781292915344,\n",
       "  1.0286641120910645,\n",
       "  0.8615967035293579,\n",
       "  0.8111891150474548,\n",
       "  0.768997073173523,\n",
       "  0.6662917137145996,\n",
       "  0.618152379989624,\n",
       "  0.6361139416694641,\n",
       "  0.5864555835723877,\n",
       "  0.5290119051933289,\n",
       "  0.5246685147285461,\n",
       "  0.49872148036956787,\n",
       "  0.4696893095970154,\n",
       "  0.4655166566371918,\n",
       "  0.45322391390800476,\n",
       "  0.41776779294013977,\n",
       "  0.3985891044139862,\n",
       "  0.4018174111843109,\n",
       "  0.39691588282585144,\n",
       "  0.37797626852989197,\n",
       "  0.3588845431804657,\n",
       "  0.3525104224681854,\n",
       "  0.35124266147613525,\n",
       "  0.3440192937850952,\n",
       "  0.333543062210083,\n",
       "  0.3224695920944214,\n",
       "  0.3150693476200104,\n",
       "  0.3102084696292877,\n",
       "  0.30381786823272705,\n",
       "  0.29714497923851013,\n",
       "  0.2894029915332794,\n",
       "  0.2820010185241699,\n",
       "  0.276216059923172,\n",
       "  0.2722039818763733,\n",
       "  0.2674105167388916,\n",
       "  0.260250061750412,\n",
       "  0.25327613949775696,\n",
       "  0.24854248762130737,\n",
       "  0.24481691420078278,\n",
       "  0.23982039093971252,\n",
       "  0.23342138528823853,\n",
       "  0.22752758860588074,\n",
       "  0.2231648862361908,\n",
       "  0.21890045702457428,\n",
       "  0.21381217241287231,\n",
       "  0.2085322141647339,\n",
       "  0.2036358267068863,\n",
       "  0.19947445392608643,\n",
       "  0.19524075090885162,\n",
       "  0.1904744952917099,\n",
       "  0.1862209439277649,\n",
       "  0.18226826190948486,\n",
       "  0.17827288806438446,\n",
       "  0.1744222342967987,\n",
       "  0.17059244215488434,\n",
       "  0.16720597445964813,\n",
       "  0.16388727724552155,\n",
       "  0.16039827466011047,\n",
       "  0.15714748203754425,\n",
       "  0.15403833985328674,\n",
       "  0.15118585526943207,\n",
       "  0.1483277976512909,\n",
       "  0.1454833298921585,\n",
       "  0.142869770526886,\n",
       "  0.14026197791099548,\n",
       "  0.13777948915958405,\n",
       "  0.13529106974601746,\n",
       "  0.13297642767429352,\n",
       "  0.13068580627441406,\n",
       "  0.12842783331871033,\n",
       "  0.12622463703155518,\n",
       "  0.12412317097187042,\n",
       "  0.12207883596420288,\n",
       "  0.12004940956830978,\n",
       "  0.11808334290981293,\n",
       "  0.1161317303776741,\n",
       "  0.11424978822469711,\n",
       "  0.11244075745344162,\n",
       "  0.1108798235654831,\n",
       "  0.11007151752710342,\n",
       "  0.11001019924879074,\n",
       "  0.10789133608341217,\n",
       "  0.10426610708236694,\n",
       "  0.10296349972486496,\n",
       "  0.10270822048187256,\n",
       "  0.1003006100654602,\n",
       "  0.0980134829878807,\n",
       "  0.09799981862306595,\n",
       "  0.09622564166784286,\n",
       "  0.09375252574682236,\n",
       "  0.09358572959899902,\n",
       "  0.09207317233085632,\n",
       "  0.09000259637832642,\n",
       "  0.08915915340185165,\n",
       "  0.08801525086164474,\n",
       "  0.08650204539299011,\n",
       "  0.0849168673157692,\n",
       "  0.08423872292041779,\n",
       "  0.08305171877145767,\n",
       "  0.08125725388526917,\n",
       "  0.08058759570121765,\n",
       "  0.07960248738527298,\n",
       "  0.07808773964643478,\n",
       "  0.07710661739110947,\n",
       "  0.07609216123819351,\n",
       "  0.07507210969924927,\n",
       "  0.07388781011104584,\n",
       "  0.07268929481506348,\n",
       "  0.07184010744094849,\n",
       "  0.07094819098711014,\n",
       "  0.06973890960216522,\n",
       "  0.06867125630378723,\n",
       "  0.06778206676244736,\n",
       "  0.06683459132909775,\n",
       "  0.06594577431678772,\n",
       "  0.06510639190673828,\n",
       "  0.06414065510034561,\n",
       "  0.06311164796352386,\n",
       "  0.06222495436668396,\n",
       "  0.06129545718431473,\n",
       "  0.060366030782461166,\n",
       "  0.059541963040828705,\n",
       "  0.05878589674830437,\n",
       "  0.058112308382987976,\n",
       "  0.057682380080223083,\n",
       "  0.05804897099733353,\n",
       "  0.059787288308143616,\n",
       "  0.06450151652097702,\n",
       "  0.07134328782558441,\n",
       "  0.07406127452850342,\n",
       "  0.06631240993738174,\n",
       "  0.054608915001153946,\n",
       "  0.06392420828342438,\n",
       "  0.05783367529511452,\n",
       "  0.05619817227125168,\n",
       "  0.058764293789863586,\n",
       "  0.05022038519382477,\n",
       "  0.05634213611483574,\n",
       "  0.048576630651950836,\n",
       "  0.05320008099079132,\n",
       "  0.048531558364629745,\n",
       "  0.049519170075654984,\n",
       "  0.04765522852540016,\n",
       "  0.047498203814029694,\n",
       "  0.04635651037096977,\n",
       "  0.04567907750606537,\n",
       "  0.0443904772400856,\n",
       "  0.04406416416168213,\n",
       "  0.04332205280661583,\n",
       "  0.042337566614151,\n",
       "  0.04199129715561867,\n",
       "  0.040278855711221695,\n",
       "  0.04064201936125755,\n",
       "  0.03905469924211502,\n",
       "  0.03919601067900658,\n",
       "  0.03779260069131851,\n",
       "  0.03771672025322914,\n",
       "  0.036515023559331894,\n",
       "  0.03660363331437111,\n",
       "  0.03535567969083786,\n",
       "  0.035274382680654526,\n",
       "  0.034357886761426926,\n",
       "  0.03394146263599396],\n",
       " [MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  MyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "    )\n",
       "  )],\n",
       " [0.06318332999944687,\n",
       "  0.1923999935388565,\n",
       "  0.20296666026115417,\n",
       "  0.44368332624435425,\n",
       "  0.5577499866485596,\n",
       "  0.6107333302497864,\n",
       "  0.7123833298683167,\n",
       "  0.7076333165168762,\n",
       "  0.652733325958252,\n",
       "  0.7163166403770447,\n",
       "  0.7233999967575073,\n",
       "  0.7597666382789612,\n",
       "  0.8072333335876465,\n",
       "  0.8092166781425476,\n",
       "  0.802383303642273,\n",
       "  0.8252666592597961,\n",
       "  0.8421833515167236,\n",
       "  0.846666693687439,\n",
       "  0.8578000068664551,\n",
       "  0.8657500147819519,\n",
       "  0.8642666935920715,\n",
       "  0.8683000206947327,\n",
       "  0.8801166415214539,\n",
       "  0.8842333555221558,\n",
       "  0.8813499808311462,\n",
       "  0.8811833262443542,\n",
       "  0.8873166441917419,\n",
       "  0.8934666514396667,\n",
       "  0.8963333368301392,\n",
       "  0.897433340549469,\n",
       "  0.8995833396911621,\n",
       "  0.9027500152587891,\n",
       "  0.9057333469390869,\n",
       "  0.908383309841156,\n",
       "  0.909766674041748,\n",
       "  0.9110166430473328,\n",
       "  0.9123833179473877,\n",
       "  0.914816677570343,\n",
       "  0.9169833064079285,\n",
       "  0.9194833040237427,\n",
       "  0.9201499819755554,\n",
       "  0.9218666553497314,\n",
       "  0.9240000247955322,\n",
       "  0.9265166521072388,\n",
       "  0.9279000163078308,\n",
       "  0.9286333322525024,\n",
       "  0.9300500154495239,\n",
       "  0.9313499927520752,\n",
       "  0.9333166480064392,\n",
       "  0.9346833229064941,\n",
       "  0.9364666938781738,\n",
       "  0.9376999735832214,\n",
       "  0.9395166635513306,\n",
       "  0.9410499930381775,\n",
       "  0.9421666860580444,\n",
       "  0.9432500004768372,\n",
       "  0.9448000192642212,\n",
       "  0.9461833238601685,\n",
       "  0.9476000070571899,\n",
       "  0.9487166404724121,\n",
       "  0.9496999979019165,\n",
       "  0.9506999850273132,\n",
       "  0.9516833424568176,\n",
       "  0.9527166485786438,\n",
       "  0.9534500241279602,\n",
       "  0.9544000029563904,\n",
       "  0.9551666378974915,\n",
       "  0.9559166431427002,\n",
       "  0.9566666483879089,\n",
       "  0.9576833248138428,\n",
       "  0.9583166837692261,\n",
       "  0.9592833518981934,\n",
       "  0.9600499868392944,\n",
       "  0.9607666730880737,\n",
       "  0.9613999724388123,\n",
       "  0.9621999859809875,\n",
       "  0.9630500078201294,\n",
       "  0.9635999798774719,\n",
       "  0.9640833139419556,\n",
       "  0.9644833207130432,\n",
       "  0.965149998664856,\n",
       "  0.9658666849136353,\n",
       "  0.9664833545684814,\n",
       "  0.9669833183288574,\n",
       "  0.9673166871070862,\n",
       "  0.9676833152770996,\n",
       "  0.9677666425704956,\n",
       "  0.9674166440963745,\n",
       "  0.9684000015258789,\n",
       "  0.9696833491325378,\n",
       "  0.9698333144187927,\n",
       "  0.9697666764259338,\n",
       "  0.9707499742507935,\n",
       "  0.9712666869163513,\n",
       "  0.9714833498001099,\n",
       "  0.9718999862670898,\n",
       "  0.9727333188056946,\n",
       "  0.9728166460990906,\n",
       "  0.9731833338737488,\n",
       "  0.9734166860580444,\n",
       "  0.9739833474159241,\n",
       "  0.974566638469696,\n",
       "  0.9747833609580994,\n",
       "  0.9753999710083008,\n",
       "  0.9756333231925964,\n",
       "  0.9759833216667175,\n",
       "  0.9766333103179932,\n",
       "  0.9767166376113892,\n",
       "  0.9770833253860474,\n",
       "  0.9775500297546387,\n",
       "  0.9779000282287598,\n",
       "  0.9780666828155518,\n",
       "  0.9783333539962769,\n",
       "  0.9786666631698608,\n",
       "  0.9791833162307739,\n",
       "  0.979533314704895,\n",
       "  0.9796000123023987,\n",
       "  0.9801833629608154,\n",
       "  0.9804333448410034,\n",
       "  0.9807166457176208,\n",
       "  0.981249988079071,\n",
       "  0.9812999963760376,\n",
       "  0.9817000031471252,\n",
       "  0.9818999767303467,\n",
       "  0.9823166728019714,\n",
       "  0.9826499819755554,\n",
       "  0.9828833341598511,\n",
       "  0.9832166433334351,\n",
       "  0.9833833575248718,\n",
       "  0.9834166765213013,\n",
       "  0.9839166402816772,\n",
       "  0.9838333129882812,\n",
       "  0.9837833046913147,\n",
       "  0.9827166795730591,\n",
       "  0.9805833101272583,\n",
       "  0.9781000018119812,\n",
       "  0.9767833352088928,\n",
       "  0.9798333048820496,\n",
       "  0.9842166900634766,\n",
       "  0.9807833433151245,\n",
       "  0.9832333326339722,\n",
       "  0.9837666749954224,\n",
       "  0.982866644859314,\n",
       "  0.98580002784729,\n",
       "  0.9836166501045227,\n",
       "  0.9863499999046326,\n",
       "  0.9845666885375977,\n",
       "  0.9865999817848206,\n",
       "  0.9865833520889282,\n",
       "  0.9867333173751831,\n",
       "  0.9867500066757202,\n",
       "  0.9874833226203918,\n",
       "  0.9874500036239624,\n",
       "  0.9876833558082581,\n",
       "  0.9879833459854126,\n",
       "  0.9884499907493591,\n",
       "  0.9886333346366882,\n",
       "  0.9886166453361511,\n",
       "  0.9895333051681519,\n",
       "  0.9892666935920715,\n",
       "  0.989883303642273,\n",
       "  0.9898666739463806,\n",
       "  0.9902166724205017,\n",
       "  0.9902333617210388,\n",
       "  0.9907166957855225,\n",
       "  0.9904166460037231,\n",
       "  0.9911666512489319,\n",
       "  0.9910333156585693,\n",
       "  0.9913666844367981,\n",
       "  0.991516649723053])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3344e-09, 2.3089e-05, 4.6382e-08, 6.6460e-02, 1.4863e-13, 9.3352e-01,\n",
      "         2.8347e-09, 4.4773e-08, 1.3687e-08, 5.7276e-07],\n",
      "        [9.9999e-01, 9.5064e-12, 5.7447e-06, 2.5536e-08, 1.2013e-11, 6.3345e-09,\n",
      "         1.0917e-07, 6.9767e-11, 2.4158e-10, 6.8196e-08],\n",
      "        [1.2172e-05, 8.8523e-08, 7.7687e-07, 8.5721e-04, 9.9355e-01, 2.9929e-04,\n",
      "         2.0395e-06, 1.9668e-04, 3.0366e-05, 5.0513e-03],\n",
      "        [1.1075e-07, 9.9992e-01, 1.4502e-06, 1.0025e-07, 2.1325e-07, 1.3974e-07,\n",
      "         6.9693e-08, 7.4759e-05, 4.3110e-06, 1.3517e-08],\n",
      "        [1.9401e-08, 6.9599e-09, 1.0342e-09, 1.9595e-05, 1.1515e-05, 7.2721e-06,\n",
      "         9.3428e-11, 4.9674e-07, 4.8959e-04, 9.9947e-01]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([5, 0, 4, 1, 9])\n",
      "tensor([5, 0, 4, 1, 9])\n",
      "tensor([7, 2, 1, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "pred = model(x_train[0:5])\n",
    "pred = torch.exp(pred)\n",
    "pred = pred/(torch.sum(pred,dim=1)).unsqueeze(1)\n",
    "print(pred)\n",
    "\n",
    "pred_idx = torch.argmax(pred,1)\n",
    "print(pred_idx)\n",
    "print(y_train[0:5])\n",
    "print(y_test[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'myMNISTmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7623b199563889390ab665eac6ce61ae3034a0dc4f90487ca20dbd35e23e2abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
